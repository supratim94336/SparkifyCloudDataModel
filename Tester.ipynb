{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "# amazon aws\n",
    "KEY = config.get('AWS', 'key')\n",
    "SECRET = config.get('AWS', 'secret')\n",
    "\n",
    "# Redshift\n",
    "DWH_DB = config.get('DWH', 'DWH_DB')\n",
    "DWH_DB_USER = config.get('DWH', 'DWH_DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('DWH', 'DWH_DB_PASSWORD')\n",
    "DWH_PORT = config.get('DWH', 'DWH_PORT')\n",
    "DWH_CLUSTER_TYPE = config.get('DWH', 'DWH_CLUSTER_TYPE')\n",
    "DWH_NUM_NODES = config.get('DWH', 'DWH_NUM_NODES')\n",
    "DWH_NODE_TYPE = config.get('DWH', 'DWH_NODE_TYPE')\n",
    "DWH_IAM_ROLE_NAME = config.get('DWH', 'DWH_IAM_ROLE_NAME')\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_SCHEMA = config.get('DWH', 'DWH_SCHEMA')\n",
    "DWH_LOG_STAGING_TABLE = config.get('DWH', 'DWH_LOG_STAGING_TABLE')\n",
    "DWH_SONG_STAGING_TABLE = config.get('DWH', 'DWH_SONG_STAGING_TABLE')\n",
    "\n",
    "# s3\n",
    "S3_BUCKET_LOG_JSON_PATH = config.get('S3', 'S3_BUCKET_LOG_JSON_PATH')\n",
    "S3_BUCKET_SONG_JSON_PATH = config.get('S3', 'S3_BUCKET_SONG_JSON_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from config import *\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "\n",
    "iam = boto3.client('iam', aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET,\n",
    "                       region_name='us-west-2'\n",
    "                       )\n",
    "try:\n",
    "    iam.create_role(Path='/',\n",
    "                    RoleName=DWH_IAM_ROLE_NAME,\n",
    "                    Description=\"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "                    AssumeRolePolicyDocument=json.dumps(\n",
    "                        {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                          'Effect': 'Allow',\n",
    "                          'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                         'Version': '2012-10-17'})\n",
    "                    )\n",
    "\n",
    "except ClientError as e:\n",
    "    print(f'ERROR: {e}')\n",
    "\n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                       )['ResponseMetadata']['HTTPStatusCode']\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Client is created ...\n",
      "1.2 Cluster config is being created ...\n",
      "1.3 Cluster is being created ...\n",
      "1.4 Cluster is created successfully ...\n"
     ]
    }
   ],
   "source": [
    "print(\"1.1 Client is created ...\")\n",
    "redshift = boto3.client('redshift',\n",
    "                        region_name=\"us-west-2\",\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET\n",
    "                        )\n",
    "try:\n",
    "    print(\"1.2 Cluster config is being created ...\")\n",
    "    redshift.create_cluster(\n",
    "        # HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        # Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "\n",
    "        # Roles (for s3 access)\n",
    "        IamRoles=[roleArn])\n",
    "except ClientError as e:\n",
    "    print(f'ERROR: {e}')\n",
    "\n",
    "print(\"1.3 Cluster is being created ...\")\n",
    "while redshift.describe_clusters(\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "        ['Clusters'][0]['ClusterStatus'] != 'available':\n",
    "    utils.animate()\n",
    "\n",
    "print(\"\\r1.4 Cluster is created successfully ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = redshift.describe_clusters(\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "    ['Clusters'][0]['Endpoint']['Address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT, DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS log_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql CREATE TABLE IF NOT EXISTS log_staging (\n",
    "        artist VARCHAR, \n",
    "        auth VARCHAR, \n",
    "        firstname VARCHAR, \n",
    "        gender VARCHAR, \n",
    "        iteminsession VARCHAR, \n",
    "        lastname VARCHAR, \n",
    "        length VARCHAR, \n",
    "        level VARCHAR, \n",
    "        location VARCHAR, \n",
    "        method VARCHAR,\n",
    "        page VARCHAR, \n",
    "        registration VARCHAR, \n",
    "        sessionId VARCHAR, \n",
    "        song VARCHAR,\n",
    "        status VARCHAR,\n",
    "        ts VARCHAR,\n",
    "        useragent VARCHAR,\n",
    "        userid VARCHAR); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "copy log_staging\n",
    "from 's3://udacity-dend/log_data/2018' \n",
    "credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
    "emptyasnull\n",
    "blanksasnull\n",
    "json 'auto'\n",
    "timeformat 'auto';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM log_staging LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT err_code, err_reason FROM STL_LOAD_ERRORS ORDER BY starttime DESC LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Client is created ...\n",
      "1.2 Cluster is identified ...\n",
      "1.3 Cluster is being deleted ...\n",
      "Please Wait ...\\"
     ]
    }
   ],
   "source": [
    "print(\"1.1 Client is created ...\")\n",
    "redshift = boto3.client('redshift',\n",
    "                        region_name=\"us-west-2\",\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET\n",
    "                        )\n",
    "print(\"1.2 Cluster is identified ...\")\n",
    "try:\n",
    "    redshift.delete_cluster(\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        SkipFinalClusterSnapshot=True)\n",
    "except ClientError as e:\n",
    "    print(f'ERROR: {e}')\n",
    "\n",
    "try:\n",
    "    print(\"1.3 Cluster is being deleted ...\")\n",
    "    while redshift.describe_clusters(\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "            ['Clusters'][0]['ClusterStatus'] == 'deleting':\n",
    "        utils.animate()\n",
    "except:\n",
    "    print(\"\\r1.4 Cluster is deleted successfully ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
