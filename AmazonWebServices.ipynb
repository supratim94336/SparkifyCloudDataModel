{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Creating Redshift Cluster using the AWS python SDK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1\n",
    "### An example of Infrastructure-as-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 0: Make sure you have an AWS secret and access key\n",
    "\n",
    "- Create a new IAM user in your AWS account\n",
    "- Give it `AdministratorAccess`, From `Attach existing policies directly` Tab\n",
    "- Take note of the access key and secret \n",
    "- Edit the file `dwh.cfg` in the same folder as this notebook and fill\n",
    "<font color='red'>\n",
    "<BR>\n",
    "[AWS]<BR>\n",
    "KEY= YOUR_AWS_KEY<BR>\n",
    "SECRET= YOUR_AWS_SECRET<BR>\n",
    "<font/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>multi-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>dwh-cluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>dwh-role</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param        Value\n",
       "0  DWH_CLUSTER_TYPE        multi-node \n",
       "1  DWH_NUM_NODES           4          \n",
       "2  DWH_NODE_TYPE           dc2.large  \n",
       "3  DWH_CLUSTER_IDENTIFIER  dwh-cluster\n",
       "4  DWH_DB                  dwh        \n",
       "5  DWH_DB_USER             dwhuser    \n",
       "6  DWH_DB_PASSWORD         Passw0rd   \n",
       "7  DWH_PORT                5439       \n",
       "8  DWH_IAM_ROLE_NAME       dwh-role   "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Creating iam client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **iam.create_role**  \n",
    "Creates a new role for your AWS account. For more information about roles, go to IAM Roles . For information about limitations on role names and the number of roles you can create, go to Limitations on IAM Entities in the IAM User Guide .\n",
    "\n",
    "### Parameters\n",
    "- **Path** (string) --\n",
    "The path to the role. For more information about paths, see IAM Identifiers in the IAM User Guide . This parameter is optional. If it is not included, it defaults to a slash (/). This parameter allows (through its regex pattern ) a string of characters consisting of either a forward slash (/) by itself or a string that must begin and end with forward slashes. In addition, it can contain any ASCII character from the ! (u0021) through the DEL character (u007F), including most punctuation characters, digits, and upper and lowercased letters.\n",
    "\n",
    "- **RoleName** (string) --\n",
    "**[REQUIRED]**\n",
    "The name of the role to create. IAM user, group, role, and policy names must be unique within the account. Names are not distinguished by case. For example, you cannot create resources named both \"MyResource\" and \"myresource\".\n",
    "\n",
    "- **AssumeRolePolicyDocument** (string) --\n",
    "**[REQUIRED]**\n",
    "The trust relationship policy document that grants an entity permission to assume the role. You must provide policies in JSON format in IAM. However, for AWS CloudFormation templates formatted in YAML, you can provide the policy in JSON or YAML format. AWS CloudFormation always converts a YAML policy to JSON format before submitting it to IAM. \n",
    "The regex pattern used to validate this parameter is a string of characters consisting of the following:-\n",
    "Any printable ASCII character ranging from the space character (u0020) through the end of the ASCII character range\n",
    "The printable characters in the Basic Latin and Latin-1 Supplement character set (through u00FF)\n",
    "The special characters tab (u0009), line feed (u000A), and carriage return (u000D)\n",
    "\n",
    "- **Description** (string) -- A description of the role.\n",
    "- **MaxSessionDuration** (integer) --\n",
    "- **The maximum session duration** (in seconds) -- that you want to set for the specified role. If you do not specify a value for this setting, the default maximum of one hour is applied. This setting can have a value from 1 hour to 12 hours. Anyone who assumes the role from the AWS CLI or API can use the DurationSeconds API parameter or the duration-seconds CLI parameter to request a longer session. The MaxSessionDuration setting determines the maximum duration that can be requested using the DurationSeconds parameter. If users don't specify a value for the DurationSeconds parameter, their security credentials are valid for one hour by default. This applies when you use the AssumeRole* API operations or the assume-role* CLI operations but does not apply when you use those operations to create a console URL. For more information, see Using IAM Roles in the IAM User Guide .\n",
    "\n",
    "- **PermissionsBoundary** (string) -- The ARN of the policy that is used to set the permissions boundary for the role.\n",
    "- **Tags**(list) --\n",
    "A list of tags that you want to attach to the newly created role. Each tag consists of a key name and an associated value. For more information about tagging, see Tagging IAM Identities in the IAM User Guide .\n",
    "\n",
    "---\n",
    "### **iam.attach_policy**\n",
    "Attaches the specified managed policy to the specified IAM group. You use this API to attach a managed policy to a group. To embed an inline policy in a group, use PutGroupPolicy.\n",
    "\n",
    "### Parameters\n",
    "**PolicyArn** (string) --\n",
    "**[REQUIRED]**\n",
    "The Amazon Resource Name (ARN) of the IAM policy you want to attach.\n",
    "\n",
    "---\n",
    "### **iam.get_role**\n",
    "Retrieves information about the specified role, including the role's path, GUID, ARN, and the role's trust policy that grants permission to assume the role. For more information about roles, see Working with Roles.\n",
    "\n",
    "### Parameters\n",
    "**RoleName** (string) --\n",
    "**[REQUIRED]**\n",
    "The name of the IAM role to get information about.\n",
    "This parameter allows (through its regex pattern ) a string of characters consisting of upper and lowercase alphanumeric characters with no spaces. You can also include any of the following characters: _+=,.@-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::764499268961:role/dwh-role\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path = '/',\n",
    "        RoleName = DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument = json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2:  Redshift Cluster\n",
    "\n",
    "- Create a RedShift Cluster\n",
    "- For complete arguments to `create_cluster`, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating redshift client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **redshift.create_cluster**  \n",
    "Creates a new cluster. To create a cluster in Virtual Private Cloud (VPC), you must provide a cluster subnet group name. The cluster subnet group identifies the subnets of your VPC that Amazon Redshift uses when creating the cluster. For more information about managing clusters, go to Amazon Redshift Clusters in the Amazon Redshift Cluster Management Guide .\n",
    "\n",
    "\n",
    "### Parameters\n",
    "**DBName** (string) --\n",
    "The name of the first database to be created when the cluster is created.\n",
    "To create additional databases after the cluster is created, connect to the cluster with a SQL client and use SQL commands to create a database. For more information, go to Create a Database in the Amazon Redshift Database Developer Guide.\n",
    "Default: dev\n",
    "*Constraints*:\n",
    "Must contain 1 to 64 alphanumeric characters.\n",
    "Must contain only lowercase letters.\n",
    "Cannot be a word that is reserved by the service. A list of reserved words can be found in Reserved Words in the Amazon Redshift Database Developer Guide.\n",
    "\n",
    "**ClusterIdentifier** (string) --\n",
    "**[REQUIRED]**\n",
    "A unique identifier for the cluster. You use this identifier to refer to the cluster for any subsequent cluster operations such as deleting or modifying. The identifier also appears in the Amazon Redshift console.\n",
    "*Constraints*:\n",
    "Must contain from 1 to 63 alphanumeric characters or hyphens.\n",
    "Alphabetic characters must be lowercase.\n",
    "First character must be a letter.\n",
    "Cannot end with a hyphen or contain two consecutive hyphens.\n",
    "Must be unique for all clusters within an AWS account.\n",
    "Example: myexamplecluster\n",
    "\n",
    "**ClusterType** (string) --\n",
    "The type of the cluster. When cluster type is specified as\n",
    "single-node , the NumberOfNodes parameter is not required.\n",
    "multi-node , the NumberOfNodes parameter is required.\n",
    "Valid Values: multi-node | single-node\n",
    "Default: multi-node\n",
    "NodeType (string) --\n",
    "**[REQUIRED]**\n",
    "The node type to be provisioned for the cluster. For information about node types, go to Working with Clusters in the Amazon Redshift Cluster Management Guide .\n",
    "Valid Values: ds2.xlarge | ds2.8xlarge | ds2.xlarge | ds2.8xlarge | dc1.large | dc1.8xlarge | dc2.large | dc2.8xlarge\n",
    "\n",
    "**MasterUsername** (string) --\n",
    "**[REQUIRED]**\n",
    "The user name associated with the master user account for the cluster that is being created.\n",
    "Constraints:\n",
    "Must be 1 - 128 alphanumeric characters. The user name can't be PUBLIC .\n",
    "First character must be a letter.\n",
    "Cannot be a reserved word. A list of reserved words can be found in Reserved Words in the Amazon Redshift Database Developer Guide.\n",
    "\n",
    "**MasterUserPassword** (string) --\n",
    "**[REQUIRED]**\n",
    "The password associated with the master user account for the cluster that is being created.\n",
    "Constraints:\n",
    "Must be between 8 and 64 characters in length.\n",
    "Must contain at least one uppercase letter.\n",
    "Must contain at least one lowercase letter.\n",
    "Must contain one number.\n",
    "Can be any printable ASCII character (ASCII code 33 to 126) except ' (single quote), \" (double quote), , /, @, or space.\n",
    "\n",
    "**ClusterSecurityGroups** (list) --\n",
    "A list of security groups to be associated with this cluster.\n",
    "Default: The default cluster security group for Amazon Redshift.\n",
    "(string) --\n",
    "\n",
    "**VpcSecurityGroupIds** (list) --\n",
    "A list of Virtual Private Cloud (VPC) security groups to be associated with the cluster.\n",
    "Default: The default VPC security group is associated with the cluster.\n",
    "(string) --\n",
    "\n",
    "**ClusterSubnetGroupName** (string) --\n",
    "The name of a cluster subnet group to be associated with this cluster.\n",
    "If this parameter is not provided the resulting cluster will be deployed outside virtual private cloud (VPC).\n",
    "\n",
    "**AvailabilityZone** (string) --\n",
    "The EC2 Availability Zone (AZ) in which you want Amazon Redshift to provision the cluster. For example, if you have several EC2 instances running in a specific Availability Zone, then you might want the cluster to be provisioned in the same zone in order to decrease network latency.\n",
    "Default: A random, system-chosen Availability Zone in the region that is specified by the endpoint.\n",
    "Example: us-east-1d\n",
    "Constraint: The specified Availability Zone must be in the same region as the current endpoint.\n",
    "\n",
    "**PreferredMaintenanceWindow** (string) --\n",
    "The weekly time range (in UTC) during which automated cluster maintenance can occur.\n",
    "Format: ddd:hh24:mi-ddd:hh24:mi\n",
    "Default: A 30-minute window selected at random from an 8-hour block of time per region, occurring on a random day of the week. For more information about the time blocks for each region, see Maintenance Windows in Amazon Redshift Cluster Management Guide.\n",
    "Valid Days: Mon | Tue | Wed | Thu | Fri | Sat | Sun\n",
    "Constraints: Minimum 30-minute window.\n",
    "\n",
    "**ClusterParameterGroupName** (string) --\n",
    "The name of the parameter group to be associated with this cluster.\n",
    "Default: The default Amazon Redshift cluster parameter group. For information about the default parameter group, go to Working with Amazon Redshift Parameter Groups\n",
    "Constraints:\n",
    "Must be 1 to 255 alphanumeric characters or hyphens.\n",
    "First character must be a letter.\n",
    "Cannot end with a hyphen or contain two consecutive hyphens.\n",
    "AutomatedSnapshotRetentionPeriod (integer) --\n",
    "The number of days that automated snapshots are retained. If the value is 0, automated snapshots are disabled. Even if automated snapshots are disabled, you can still create manual snapshots when you want with CreateClusterSnapshot.\n",
    "Default: 1\n",
    "Constraints: Must be a value from 0 to 35.\n",
    "\n",
    "**ManualSnapshotRetentionPeriod** (integer) --\n",
    "The default number of days to retain a manual snapshot. If the value is -1, the snapshot is retained indefinitely. This setting doesn't change the retention period of existing snapshots.\n",
    "The value must be either -1 or an integer between 1 and 3,653.\n",
    "\n",
    "**Port (integer)** --\n",
    "The port number on which the cluster accepts incoming connections.\n",
    "The cluster is accessible only via the JDBC and ODBC connection strings. Part of the connection string requires the port on which the cluster will listen for incoming connections.\n",
    "Default: 5439\n",
    "Valid Values: 1150-65535\n",
    "\n",
    "**ClusterVersion (string)** --\n",
    "The version of the Amazon Redshift engine software that you want to deploy on the cluster.\n",
    "The version selected runs on all the nodes in the cluster.\n",
    "Constraints: Only version 1.0 is currently available.\n",
    "Example: 1.0\n",
    "\n",
    "**AllowVersionUpgrade** (boolean) --\n",
    "If true , major version upgrades can be applied during the maintenance window to the Amazon Redshift engine that is running on the cluster.\n",
    "When a new major version of the Amazon Redshift engine is released, you can request that the service automatically apply upgrades during the maintenance window to the Amazon Redshift engine that is running on your cluster.\n",
    "Default: true\n",
    "\n",
    "**NumberOfNodes** (integer) --\n",
    "The number of compute nodes in the cluster. This parameter is required when the ClusterType parameter is specified as multi-node .\n",
    "For information about determining how many nodes you need, go to Working with Clusters in the Amazon Redshift Cluster Management Guide .\n",
    "If you don't specify this parameter, you get a single-node cluster. When requesting a multi-node cluster, you must specify the number of nodes that you want in the cluster.\n",
    "Default: 1\n",
    "Constraints: Value must be at least 1 and no more than 100.\n",
    "\n",
    "**PubliclyAccessible** (boolean) -- If true , the cluster can be accessed from a public network.\n",
    "Encrypted (boolean) --\n",
    "If true , the data in the cluster is encrypted at rest.\n",
    "Default: false\n",
    "\n",
    "**HsmClientCertificateIdentifier** (string) -- Specifies the name of the HSM client certificate the Amazon Redshift cluster uses to retrieve the data encryption keys stored in an HSM.\n",
    "\n",
    "**HsmConfigurationIdentifier** (string) -- Specifies the name of the HSM configuration that contains the information the Amazon Redshift cluster can use to retrieve and store keys in an HSM.\n",
    "ElasticIp (string) --\n",
    "The Elastic IP (EIP) address for the cluster.\n",
    "Constraints: The cluster must be provisioned in EC2-VPC and publicly-accessible through an Internet gateway. For more information about provisioning clusters in EC2-VPC, go to Supported Platforms to Launch Your Cluster in the Amazon Redshift Cluster Management Guide.\n",
    "\n",
    "**Tags** (list) --\n",
    "A list of tag instances.\n",
    "(dict) --\n",
    "A tag consisting of a name/value pair for a resource.\n",
    "Key (string) --\n",
    "The key, or name, for the resource tag.\n",
    "Value (string) --\n",
    "The value for the resource tag.\n",
    "\n",
    "**KmsKeyId** (string) -- The AWS Key Management Service (KMS) key ID of the encryption key that you want to use to encrypt data in the cluster.\n",
    "EnhancedVpcRouting (boolean) --\n",
    "An option that specifies whether to create the cluster with enhanced VPC routing enabled. To create a cluster that uses enhanced VPC routing, the cluster must be in a VPC. For more information, see Enhanced VPC Routing in the Amazon Redshift Cluster Management Guide.\n",
    "If this option is true , enhanced VPC routing is enabled.\n",
    "Default: false\n",
    "\n",
    "**AdditionalInfo** (string) -- Reserved.\n",
    "\n",
    "**IamRoles** (list) --\n",
    "A list of AWS Identity and Access Management (IAM) roles that can be used by the cluster to access other AWS services. You must supply the IAM roles in their Amazon Resource Name (ARN) format. You can supply up to 10 IAM roles in a single request.\n",
    "A cluster can have up to 10 IAM roles associated with it at any time.\n",
    "(string) --\n",
    "\n",
    "**MaintenanceTrackName** (string) -- An optional parameter for the name of the maintenance track for the cluster. If you don't provide a maintenance track name, the cluster is assigned to the current track.\n",
    "\n",
    "**SnapshotScheduleIdentifier** (string) -- A unique identifier for the snapshot schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **redshift.describe_clusters**  \n",
    "Returns properties of provisioned clusters including general cluster properties, cluster database properties, maintenance and backup properties, and security and access properties. This operation supports pagination. For more information about managing clusters, go to Amazon Redshift Clusters in the Amazon Redshift Cluster Management Guide .\n",
    "\n",
    "If you specify both tag keys and tag values in the same request, Amazon Redshift returns all clusters that match any combination of the specified keys and values. For example, if you have owner and environment for tag keys, and admin and test for tag values, all clusters that have any combination of those values are returned.\n",
    "\n",
    "If both tag keys and values are omitted from the request, clusters are returned regardless of whether they have tag keys or values associated with them.\n",
    "\n",
    "### Parameters\n",
    "**ClusterIdentifier** (string) --\n",
    "The unique identifier of a cluster whose properties you are requesting. This parameter is case sensitive.\n",
    "\n",
    "The default is that all clusters defined for an account are returned.\n",
    "\n",
    "**MaxRecords** (integer) --\n",
    "The maximum number of response records to return in each call. If the number of remaining response records exceeds the specified MaxRecords value, a value is returned in a marker field of the response. You can retrieve the next set of records by retrying the command with the returned marker value.\n",
    "Default: 100\n",
    "Constraints: minimum 20, maximum 100.\n",
    "\n",
    "**Marker** (string) --\n",
    "An optional parameter that specifies the starting point to return a set of response records. When the results of a DescribeClusters request exceed the value specified in MaxRecords , AWS returns a value in the Marker field of the response. You can retrieve the next set of response records by providing the returned marker value in the Marker parameter and retrying the request.\n",
    "Constraints: You can specify either the ClusterIdentifier parameter or the Marker parameter, but not both.\n",
    "\n",
    "**TagKeys** (list) --\n",
    "A tag key or keys for which you want to return all matching clusters that are associated with the specified key or keys. For example, suppose that you have clusters that are tagged with keys called owner and environment . If you specify both of these tag keys in the request, Amazon Redshift returns a response with the clusters that have either or both of these tag keys associated with them.\n",
    "(string) --\n",
    "\n",
    "**TagValues** (list) --\n",
    "A tag value or values for which you want to return all matching clusters that are associated with the specified tag value or values. For example, suppose that you have clusters that are tagged with values called admin and test . If you specify both of these tag values in the request, Amazon Redshift returns a response with the clusters that have either or both of these tag values associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwh-cluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-ce44eeb6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                    Value  \n",
       "0  dwh-cluster                                                                             \n",
       "1  dc2.large                                                                               \n",
       "2  available                                                                               \n",
       "3  dwhuser                                                                                 \n",
       "4  dwh                                                                                     \n",
       "5  {'Address': 'dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-ce44eeb6                                                                            \n",
       "7  4                                                                                       "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ClusterIdentifier': 'dwh-cluster',\n",
       " 'NodeType': 'dc2.large',\n",
       " 'ClusterStatus': 'available',\n",
       " 'MasterUsername': 'dwhuser',\n",
       " 'DBName': 'dwh',\n",
       " 'Endpoint': {'Address': 'dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com',\n",
       "  'Port': 5439},\n",
       " 'ClusterCreateTime': datetime.datetime(2019, 6, 1, 12, 38, 30, 77000, tzinfo=tzutc()),\n",
       " 'AutomatedSnapshotRetentionPeriod': 1,\n",
       " 'ManualSnapshotRetentionPeriod': -1,\n",
       " 'ClusterSecurityGroups': [],\n",
       " 'VpcSecurityGroups': [],\n",
       " 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "   'ParameterApplyStatus': 'in-sync'}],\n",
       " 'ClusterSubnetGroupName': 'default',\n",
       " 'VpcId': 'vpc-ce44eeb6',\n",
       " 'AvailabilityZone': 'us-west-2c',\n",
       " 'PreferredMaintenanceWindow': 'fri:07:00-fri:07:30',\n",
       " 'PendingModifiedValues': {},\n",
       " 'ClusterVersion': '1.0',\n",
       " 'AllowVersionUpgrade': True,\n",
       " 'NumberOfNodes': 4,\n",
       " 'PubliclyAccessible': True,\n",
       " 'Encrypted': False,\n",
       " 'ClusterPublicKey': 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCtyUAUoOAlGcYYEa98AVObSefDSLQK2/fdVTuuvKyXXpR2QDvXOeqMpwvQYa4Sup8cnkvhGRgi0sYhGgeBtsRd/yVNGrZsQjM8AHgLe9G/Z7+MaFtZKZn8vdRme4o2P3jBq8Mg2cnmrW7HsECLUN60GxDH2uOBUCn0FJPnBr46+avPGP4o4CyhewqxQ5pXhUZaIWpx37rFWBpVBe+/POFEey77baKxJYZbZ0dXcCNomAEfosYQNeOuPeLsteyn7Z9LdRjl0e8iwSwMBnraDOtaNWk+wuNu26gFeDQsAe0VNwUnTnlVqhNWtqPATFbxaTfWWF4SG6bBd7tSXzzjySdX Amazon-Redshift\\n',\n",
       " 'ClusterNodes': [{'NodeRole': 'LEADER',\n",
       "   'PrivateIPAddress': '172.31.3.155',\n",
       "   'PublicIPAddress': '54.214.209.104'},\n",
       "  {'NodeRole': 'COMPUTE-0',\n",
       "   'PrivateIPAddress': '172.31.3.157',\n",
       "   'PublicIPAddress': '35.161.4.159'},\n",
       "  {'NodeRole': 'COMPUTE-1',\n",
       "   'PrivateIPAddress': '172.31.3.49',\n",
       "   'PublicIPAddress': '34.213.212.80'},\n",
       "  {'NodeRole': 'COMPUTE-2',\n",
       "   'PrivateIPAddress': '172.31.11.223',\n",
       "   'PublicIPAddress': '34.216.10.20'},\n",
       "  {'NodeRole': 'COMPUTE-3',\n",
       "   'PrivateIPAddress': '172.31.2.124',\n",
       "   'PublicIPAddress': '35.161.127.108'}],\n",
       " 'ClusterRevisionNumber': '7804',\n",
       " 'Tags': [],\n",
       " 'EnhancedVpcRouting': False,\n",
       " 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::764499268961:role/dwh-role',\n",
       "   'ApplyStatus': 'in-sync'}],\n",
       " 'MaintenanceTrackName': 'current',\n",
       " 'ElasticResizeNumberOfNodeOptions': '[2,8]',\n",
       " 'DeferredMaintenanceWindows': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myClusterProps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take note of the cluster endpoint and role ARN  \n",
    "<font color='red'>DO NOT RUN THIS unless the cluster status becomes \"Available\" </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::764499268961:role/dwh-role\n"
     ]
    }
   ],
   "source": [
    "# endpoint\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", endpoint)\n",
    "print(\"DWH_ROLE_ARN :: \", roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Create an endpoint to access the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class EC2.Vpc(id)\n",
    "A resource representing an Amazon Elastic Compute Cloud (EC2) Vpc\n",
    "\n",
    "---\n",
    "### **security_groups**  \n",
    "\n",
    "A collection of SecurityGroup resources  \n",
    "\n",
    "---\n",
    "### **all()**  \n",
    "\n",
    "Creates an iterable of all SecurityGroup resources in the collection.  \n",
    "\n",
    "---\n",
    "### **authorize_ingress**  \n",
    "Adds the specified ingress rules to a security group.\n",
    "An inbound rule permits instances to receive traffic from the specified IPv4 or IPv6 CIDR address ranges, or from the instances associated with the specified destination security groups.\n",
    "You specify a protocol for each rule (for example, TCP). For TCP and UDP, you must also specify the destination port or port range. For ICMP/ICMPv6, you must also specify the ICMP/ICMPv6 type and code. You can use -1 to mean all types or all codes.\n",
    "Rule changes are propagated to instances within the security group as quickly as possible. However, a small delay might occur.\n",
    "\n",
    "### Parameters\n",
    "**CidrIp** (string) --\n",
    "The IPv4 address range, in CIDR format. You can't specify this parameter when specifying a source security group. To specify an IPv6 address range, use a set of IP permissions.\n",
    "Alternatively, use a set of IP permissions to specify multiple rules and a description for the rule.\n",
    "\n",
    "**FromPort** (integer) --\n",
    "The start of port range for the TCP and UDP protocols, or an ICMP type number. For the ICMP type number, use -1 to specify all types. If you specify all ICMP types, you must specify all codes.\n",
    "Alternatively, use a set of IP permissions to specify multiple rules and a description for the rule.\n",
    "\n",
    "**GroupName** (string) -- [EC2-Classic, default VPC] The name of the security group. You must specify either the security group ID or the security group name in the request.\n",
    "IpPermissions (list) --\n",
    "The sets of IP permissions.\n",
    "(dict) --\n",
    "Describes a set of permissions for a security group rule.\n",
    "\n",
    "**FromPort** (integer) --\n",
    "The start of port range for the TCP and UDP protocols, or an ICMP/ICMPv6 type number. A value of -1 indicates all ICMP/ICMPv6 types. If you specify all ICMP/ICMPv6 types, you must specify all codes.\n",
    "\n",
    "**IpProtocol** (string) --\n",
    "The IP protocol name (tcp , udp , icmp , icmpv6 ) or number (see Protocol Numbers ).\n",
    "\n",
    "[VPC only] Use -1 to specify all protocols. When authorizing security group rules, specifying -1 or a protocol number other than tcp , udp , icmp , or icmpv6 allows traffic on all ports, regardless of any port range you specify. For tcp , udp , and icmp , you must specify a port range. For icmpv6 , the port range is optional; if you omit the port range, traffic for all types and codes is allowed.\n",
    "\n",
    "**IpRanges** (list) --\n",
    "The IPv4 ranges.\n",
    "(dict) --\n",
    "Describes an IPv4 range.\n",
    "\n",
    "**CidrIp** (string) --\n",
    "The IPv4 CIDR range. You can either specify a CIDR range or a source security group, not both. To specify a single IPv4 address, use the /32 prefix length.\n",
    "\n",
    "**Description** (string) --\n",
    "A description for the security group rule that references this IPv4 address range.\n",
    "Constraints: Up to 255 characters in length. Allowed characters are a-z, A-Z, 0-9, spaces, and ._-:/()#,@[]+=;{}!$*\n",
    "\n",
    "**Ipv6Ranges** (list) --\n",
    "[VPC only] The IPv6 ranges.\n",
    "(dict) --\n",
    "[EC2-VPC only] Describes an IPv6 range.\n",
    "\n",
    "**CidrIpv6** (string) --\n",
    "The IPv6 CIDR range. You can either specify a CIDR range or a source security group, not both. To specify a single IPv6 address, use the /128 prefix length.\n",
    "\n",
    "**Description** (string) --\n",
    "A description for the security group rule that references this IPv6 address range.\n",
    "Constraints: Up to 255 characters in length. Allowed characters are a-z, A-Z, 0-9, spaces, and ._-:/()#,@[]+=;{}!$*\n",
    "\n",
    "**PrefixListIds** (list) --\n",
    "[VPC only] The prefix list IDs for an AWS service. With outbound rules, this is the AWS service to access through a VPC endpoint from instances associated with the security group.\n",
    "(dict) --\n",
    "Describes a prefix list ID.\n",
    "\n",
    "**Description** (string) --\n",
    "A description for the security group rule that references this prefix list ID.\n",
    "Constraints: Up to 255 characters in length. Allowed characters are a-z, A-Z, 0-9, spaces, and ._-:/()#,@[]+=;{}!$*\n",
    "\n",
    "**PrefixListId** (string) --\n",
    "The ID of the prefix.\n",
    "\n",
    "**ToPort** (integer) --\n",
    "The end of port range for the TCP and UDP protocols, or an ICMP/ICMPv6 code. A value of -1 indicates all ICMP/ICMPv6 codes. If you specify all ICMP/ICMPv6 types, you must specify all codes.\n",
    "\n",
    "**UserIdGroupPairs** (list) --\n",
    "The security group and AWS account ID pairs.\n",
    "(dict) --\n",
    "Describes a security group and AWS account ID pair.\n",
    "\n",
    "**Description** (string) --\n",
    "A description for the security group rule that references this user ID group pair.\n",
    "Constraints: Up to 255 characters in length. Allowed characters are a-z, A-Z, 0-9, spaces, and ._-:/()#,@[]+=;{}!$*\n",
    "\n",
    "**GroupId**(string) --\n",
    "The ID of the security group.\n",
    "\n",
    "**GroupName** (string) --\n",
    "The name of the security group. In a request, use this parameter for a security group in EC2-Classic or a default VPC only. For a security group in a nondefault VPC, use the security group ID.\n",
    "For a referenced security group in another VPC, this value is not returned if the referenced security group is deleted.\n",
    "\n",
    "**PeeringStatus** (string) --\n",
    "The status of a VPC peering connection, if applicable.\n",
    "\n",
    "**UserId** (string) --\n",
    "The ID of an AWS account.\n",
    "For a referenced security group in another VPC, the account ID of the referenced security group is returned in the response. If the referenced security group is deleted, this value is not returned.\n",
    "[EC2-Classic] Required when adding or removing rules that reference a security group in another AWS account.\n",
    "\n",
    "**VpcId** (string) --\n",
    "The ID of the VPC for the referenced security group, if applicable.\n",
    "\n",
    "**VpcPeeringConnectionId** (string) --\n",
    "The ID of the VPC peering connection, if applicable.\n",
    "\n",
    "**IpProtocol** (string) --\n",
    "The IP protocol name (tcp , udp , icmp ) or number (see Protocol Numbers ). To specify icmpv6 , use a set of IP permissions.\n",
    "[VPC only] Use -1 to specify all protocols. If you specify -1 or a protocol other than tcp , udp , or icmp , traffic on all ports is allowed, regardless of any ports you specify.\n",
    "Alternatively, use a set of IP permissions to specify multiple rules and a description for the rule.\n",
    "\n",
    "**SourceSecurityGroupName** (string) -- [EC2-Classic, default VPC] The name of the source security group. You can't specify this parameter in combination with the following parameters: the CIDR IP address range, the start of the port range, the IP protocol, and the end of the port range. Creates rules that grant full ICMP, UDP, and TCP access. To create a rule with a specific IP protocol and port range, use a set of IP permissions instead. For EC2-VPC, the source security group must be in the same VPC.\n",
    "\n",
    "**SourceSecurityGroupOwnerId** (string) -- [nondefault VPC] The AWS account ID for the source security group, if the source security group is in a different account. You can't specify this parameter in combination with the following parameters: the CIDR IP address range, the IP protocol, the start of the port range, and the end of the port range. Creates rules that grant full ICMP, UDP, and TCP access. To create a rule with a specific IP protocol and port range, use a set of IP permissions instead.\n",
    "\n",
    "**ToPort** (integer) --\n",
    "The end of port range for the TCP and UDP protocols, or an ICMP code number. For the ICMP code number, use -1 to specify all codes. If you specify all ICMP types, you must specify all codes.\n",
    "Alternatively, use a set of IP permissions to specify multiple rules and a description for the rule.\n",
    "\n",
    "**DryRun** (boolean) -- Checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation . Otherwise, it is UnauthorizedOperation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-d499ea99')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    #print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "   )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use sql client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT, DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Create S3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/full/')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/full/full.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00000-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00001-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00002-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00003-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00004-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00005-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00006-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00007-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00008-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n",
      "s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00009-d33afb94-b8af-407d-abd5-59c0ee8f5ee8-c000.csv.gz')\n"
     ]
    }
   ],
   "source": [
    "# load udacity bucket\n",
    "udacity_labs_bucket = s3.Bucket('udacity-labs')\n",
    "for obj in udacity_labs_bucket.objects.filter(Prefix='tickets'):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redshift Data Types\n",
    "- SMALLINT (INT2)\n",
    "- INTEGER (INT, INT4)\n",
    "- BIGINT (INT8)\n",
    "- DECIMAL (NUMERIC)\n",
    "- REAL (FLOAT4)\n",
    "- DOUBLE PRECISION (FLOAT8)\n",
    "- BOOLEAN (BOOL)\n",
    "- CHAR (CHARACTER)\n",
    "- VARCHAR (CHARACTER VARYING)\n",
    "- DATE\n",
    "- TIMESTAMP\n",
    "- TIMESTAMPTZ\n",
    "- SERIAL (DEFAULT nextval('name_of_the_variable') NOT NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"sporting_event_ticket\";\n",
    "CREATE TABLE \"sporting_event_ticket\" (\n",
    "    \"id\" double precision DEFAULT nextval('sporting_event_ticket_seq') NOT NULL,\n",
    "    \"sporting_event_id\" double precision NOT NULL,\n",
    "    \"sport_location_id\" double precision NOT NULL,\n",
    "    \"seat_level\" numeric(1,0) NOT NULL,\n",
    "    \"seat_section\" character varying(15) NOT NULL,\n",
    "    \"seat_row\" character varying(10) NOT NULL,\n",
    "    \"seat\" character varying(10) NOT NULL,\n",
    "    \"ticketholder_id\" double precision,\n",
    "    \"ticket_price\" numeric(8,2) NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Partitioned data into the cluster\n",
    "Use the COPY command to load data from `s3://udacity-labs/tickets/split/part` using your iam role credentials. Use gzip delimiter `;`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::764499268961:role/dwh-role'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DWH_ROLE_ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 4.53 ms, sys: 3.9 ms, total: 8.42 ms\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "      copy sporting_event_ticket \n",
    "      from 's3://udacity-labs/tickets/split/part'\n",
    "      credentials 'aws_iam_role={}'\n",
    "      gzip DELIMITER ';'\n",
    "      compupdate off\n",
    "      region 'us-west-2';\n",
    "      \"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"sporting_event_ticket_full\";\n",
    "CREATE TABLE \"sporting_event_ticket_full\" (\n",
    "    \"id\" double precision DEFAULT nextval('sporting_event_ticket_seq') NOT NULL,\n",
    "    \"sporting_event_id\" double precision NOT NULL,\n",
    "    \"sport_location_id\" double precision NOT NULL,\n",
    "    \"seat_level\" numeric(1,0) NOT NULL,\n",
    "    \"seat_section\" character varying(15) NOT NULL,\n",
    "    \"seat_row\" character varying(10) NOT NULL,\n",
    "    \"seat\" character varying(10) NOT NULL,\n",
    "    \"ticketholder_id\" double precision,\n",
    "    \"ticket_price\" numeric(8,2) NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting data full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "CPU times: user 4.33 ms, sys: 2.52 ms, total: 6.85 ms\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "      copy sporting_event_ticket_full\n",
    "      from 's3://udacity-labs/tickets/full/full.csv.gz'\n",
    "      credentials 'aws_iam_role={}'\n",
    "      gzip DELIMITER ';'\n",
    "      compupdate off\n",
    "      region 'us-west-2';\n",
    "      \"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's work with different kinds of distribution strategies and compare them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to use a benchmarking data set common for benchmarking star schemas in data warehouses.\n",
    "- The data is pre-loaded in a public bucket on the `us-west-2` region\n",
    "- Our examples will be based on the Amazon Redshfit tutorial but in a scripted environment in our workspace.\n",
    "\n",
    "<img src=\"./img/aws-schema.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No distribution schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP SCHEMA IF EXISTS nodist CASCADE;\n",
    "CREATE SCHEMA nodist;\n",
    "SET search_path TO nodist;\n",
    "\n",
    "DROP TABLE IF EXISTS part cascade;\n",
    "DROP TABLE IF EXISTS supplier;\n",
    "DROP TABLE IF EXISTS supplier;\n",
    "DROP TABLE IF EXISTS customer;\n",
    "DROP TABLE IF EXISTS dwdate;\n",
    "DROP TABLE IF EXISTS lineorder;\n",
    "\n",
    "CREATE TABLE part \n",
    "(\n",
    "  p_partkey       INTEGER     NOT NULL,\n",
    "  p_name          VARCHAR(22) NOT NULL,\n",
    "  p_mfgr          VARCHAR(6)  NOT NULL,\n",
    "  p_category      VARCHAR(7)  NOT NULL,\n",
    "  p_brand1        VARCHAR(9)  NOT NULL,\n",
    "  p_color         VARCHAR(11) NOT NULL,\n",
    "  p_type          VARCHAR(25) NOT NULL,\n",
    "  p_size          INTEGER     NOT NULL,\n",
    "  p_container     VARCHAR(10) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE supplier \n",
    "(\n",
    "    s_suppkey     INTEGER     NOT NULL,\n",
    "    s_name        VARCHAR(50) NOT NULL,\n",
    "    s_address     VARCHAR(50) NOT NULL,\n",
    "    s_city        VARCHAR(30) NOT NULL,\n",
    "    s_nation      VARCHAR(30) NOT NULL,\n",
    "    s_region      VARCHAR(30) NOT NULL,\n",
    "    s_phone       VARCHAR(20) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE customer \n",
    "(\n",
    "    c_custkey     INTEGER     NOT NULL,\n",
    "    c_name        VARCHAR(50) NOT NULL,\n",
    "    c_address     VARCHAR(30) NOT NULL,\n",
    "    c_city        VARCHAR(30) NOT NULL,\n",
    "    c_nation      VARCHAR(30) NOT NULL,\n",
    "    c_region      VARCHAR(30) NOT NULL,\n",
    "    c_phone       VARCHAR(30) NOT NULL,\n",
    "    c_mktsegment  VARCHAR(30) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE dwdate \n",
    "(\n",
    "    d_datekey          INTEGER     NOT NULL,\n",
    "    d_date             VARCHAR(20) NOT NULL,\n",
    "    d_dayofweek        VARCHAR(20) NOT NULL,\n",
    "    d_month            VARCHAR(20) NOT NULL,\n",
    "    d_year             INTEGER     NOT NULL,\n",
    "    d_yearmonthnum     INTEGER     NOT NULL,\n",
    "    d_yearmonth        VARCHAR(10) NOT NULL,\n",
    "    d_daynuminweek     INTEGER     NOT NULL,\n",
    "    d_daynuminmonth    INTEGER     NOT NULL,\n",
    "    d_daynuminyear     INTEGER     NOT NULL,\n",
    "    d_monthnuminyear   INTEGER     NOT NULL,\n",
    "    d_weeknuminyear    INTEGER     NOT NULL,\n",
    "    d_sellingseason    VARCHAR(20) NOT NULL,\n",
    "    d_lastdayinweekfl  VARCHAR(1)  NOT NULL,\n",
    "    d_lastdayinmonthfl VARCHAR(1)  NOT NULL,\n",
    "    d_holidayfl        VARCHAR(1)  NOT NULL,\n",
    "    d_weekdayfl        VARCHAR(1)  NOT NULL\n",
    ");\n",
    "CREATE TABLE lineorder \n",
    "(\n",
    "    lo_orderkey        INTEGER     NOT NULL,\n",
    "    lo_linenumber      INTEGER     NOT NULL,\n",
    "    lo_custkey         INTEGER     NOT NULL,\n",
    "    lo_partkey         INTEGER     NOT NULL,\n",
    "    lo_suppkey         INTEGER     NOT NULL,\n",
    "    lo_orderdate       VARCHAR(20) NOT NULL,\n",
    "    lo_orderpriority   VARCHAR(20) NOT NULL,\n",
    "    lo_shippriority    VARCHAR(20) NOT NULL,\n",
    "    lo_quantity        INTEGER     NOT NULL,\n",
    "    lo_extendedprice   INTEGER     NOT NULL,\n",
    "    lo_ordertotalprice INTEGER     NOT NULL,\n",
    "    lo_discount        INTEGER     NOT NULL,\n",
    "    lo_revenue         INTEGER     NOT NULL,\n",
    "    lo_supplycost      INTEGER     NOT NULL,\n",
    "    lo_tax             INTEGER     NOT NULL,\n",
    "    lo_commitdate        VARCHAR(20) NOT NULL,\n",
    "    lo_shipmode        VARCHAR(20) NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dist schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "DROP SCHEMA IF EXISTS dist CASCADE;\n",
    "CREATE SCHEMA dist;\n",
    "SET search_path TO dist;\n",
    "\n",
    "DROP TABLE IF EXISTS part cascade;\n",
    "DROP TABLE IF EXISTS supplier;\n",
    "DROP TABLE IF EXISTS supplier;\n",
    "DROP TABLE IF EXISTS customer;\n",
    "DROP TABLE IF EXISTS dwdate;\n",
    "DROP TABLE IF EXISTS lineorder;\n",
    "\n",
    "CREATE TABLE part \n",
    "(\n",
    "  p_partkey       INTEGER     NOT NULL    sortkey distkey,\n",
    "  p_name          VARCHAR(22) NOT NULL,\n",
    "  p_mfgr          VARCHAR(6)  NOT NULL,\n",
    "  p_category      VARCHAR(7)  NOT NULL,\n",
    "  p_brand1        VARCHAR(9)  NOT NULL,\n",
    "  p_color         VARCHAR(11) NOT NULL,\n",
    "  p_type          VARCHAR(25) NOT NULL,\n",
    "  p_size          INTEGER     NOT NULL,\n",
    "  p_container     VARCHAR(10) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE supplier \n",
    "(\n",
    "    s_suppkey     INTEGER     NOT NULL    sortkey,\n",
    "    s_name        VARCHAR(50) NOT NULL,\n",
    "    s_address     VARCHAR(50) NOT NULL,\n",
    "    s_city        VARCHAR(30) NOT NULL,\n",
    "    s_nation      VARCHAR(30) NOT NULL,\n",
    "    s_region      VARCHAR(30) NOT NULL,\n",
    "    s_phone       VARCHAR(20) NOT NULL\n",
    ") diststyle all;\n",
    "\n",
    "CREATE TABLE customer \n",
    "(\n",
    "    c_custkey     INTEGER     NOT NULL    sortkey,\n",
    "    c_name        VARCHAR(50) NOT NULL,\n",
    "    c_address     VARCHAR(30) NOT NULL,\n",
    "    c_city        VARCHAR(30) NOT NULL,\n",
    "    c_nation      VARCHAR(30) NOT NULL,\n",
    "    c_region      VARCHAR(30) NOT NULL,\n",
    "    c_phone       VARCHAR(30) NOT NULL,\n",
    "    c_mktsegment  VARCHAR(30) NOT NULL\n",
    ") diststyle all;\n",
    "\n",
    "CREATE TABLE dwdate \n",
    "(\n",
    "    d_datekey          INTEGER     NOT NULL   sortkey,\n",
    "    d_date             VARCHAR(20) NOT NULL,\n",
    "    d_dayofweek        VARCHAR(20) NOT NULL,\n",
    "    d_month            VARCHAR(20) NOT NULL,\n",
    "    d_year             INTEGER     NOT NULL,\n",
    "    d_yearmonthnum     INTEGER     NOT NULL,\n",
    "    d_yearmonth        VARCHAR(10) NOT NULL,\n",
    "    d_daynuminweek     INTEGER     NOT NULL,\n",
    "    d_daynuminmonth    INTEGER     NOT NULL,\n",
    "    d_daynuminyear     INTEGER     NOT NULL,\n",
    "    d_monthnuminyear   INTEGER     NOT NULL,\n",
    "    d_weeknuminyear    INTEGER     NOT NULL,\n",
    "    d_sellingseason    VARCHAR(20) NOT NULL,\n",
    "    d_lastdayinweekfl  VARCHAR(1)  NOT NULL,\n",
    "    d_lastdayinmonthfl VARCHAR(1)  NOT NULL,\n",
    "    d_holidayfl        VARCHAR(1)  NOT NULL,\n",
    "    d_weekdayfl        VARCHAR(1)  NOT NULL\n",
    ") diststyle all;\n",
    "\n",
    "CREATE TABLE lineorder \n",
    "(\n",
    "    lo_orderkey        INTEGER     NOT NULL,\n",
    "    lo_linenumber      INTEGER     NOT NULL,\n",
    "    lo_custkey         INTEGER     NOT NULL,\n",
    "    lo_partkey         INTEGER     NOT NULL   distkey,\n",
    "    lo_suppkey         INTEGER     NOT NULL,\n",
    "    lo_orderdate       INTEGER     NOT NULL   sortkey,\n",
    "    lo_orderpriority   VARCHAR(20) NOT NULL,\n",
    "    lo_shippriority    VARCHAR(20) NOT NULL,\n",
    "    lo_quantity        INTEGER     NOT NULL,\n",
    "    lo_extendedprice   INTEGER     NOT NULL,\n",
    "    lo_ordertotalprice INTEGER     NOT NULL,\n",
    "    lo_discount        INTEGER     NOT NULL,\n",
    "    lo_revenue         INTEGER     NOT NULL,\n",
    "    lo_supplycost      INTEGER     NOT NULL,\n",
    "    lo_tax             INTEGER     NOT NULL,\n",
    "    lo_commitdate        VARCHAR(20) NOT NULL,\n",
    "    lo_shipmode        VARCHAR(20) NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying data into table\n",
    "# %sql is to run a standalone sql command\n",
    "# %%sql is to run a whole block as a sql block no more python\n",
    "def loadTables(schema, tables):\n",
    "    loadTimes = []\n",
    "    SQL_SET_SCEMA = \"SET search_path TO {};\".format(schema)\n",
    "    %sql $SQL_SET_SCEMA\n",
    "    \n",
    "    for table in tables:\n",
    "        SQL_COPY  = \"\"\"\n",
    "                    copy {} from 's3://awssampledbuswest2/ssbgz/{}' \n",
    "                    credentials 'aws_iam_role={}'\n",
    "                    gzip region 'us-west-2';\n",
    "                    \"\"\".format(table, table, DWH_ROLE_ARN)\n",
    "\n",
    "        print(\"======= LOADING TABLE: ** {} ** IN SCHEMA ==> {} =======\".format(table, schema))\n",
    "        print(SQL_COPY)\n",
    "\n",
    "        t0 = time()\n",
    "        %sql $SQL_COPY\n",
    "        loadTime = time() - t0\n",
    "        loadTimes.append(loadTime)\n",
    "\n",
    "        print(\"=== DONE IN: {0:.2f} sec\\n\".format(loadTime))\n",
    "    return pd.DataFrame({\"table\":tables, \"loadtime_\"+schema:loadTimes}).set_index('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the tables to be loaded\n",
    "tables = [\"customer\", \"dwdate\", \"supplier\", \"part\", \"lineorder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "======= LOADING TABLE: ** customer ** IN SCHEMA ==> nodist =======\n",
      "\n",
      "                   copy customer from 's3://awssampledbuswest2/ssbgz/customer' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 13.98 sec\n",
      "\n",
      "======= LOADING TABLE: ** dwdate ** IN SCHEMA ==> nodist =======\n",
      "\n",
      "                   copy dwdate from 's3://awssampledbuswest2/ssbgz/dwdate' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 1.45 sec\n",
      "\n",
      "======= LOADING TABLE: ** supplier ** IN SCHEMA ==> nodist =======\n",
      "\n",
      "                   copy supplier from 's3://awssampledbuswest2/ssbgz/supplier' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 7.56 sec\n",
      "\n",
      "======= LOADING TABLE: ** part ** IN SCHEMA ==> nodist =======\n",
      "\n",
      "                   copy part from 's3://awssampledbuswest2/ssbgz/part' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 5.22 sec\n",
      "\n",
      "======= LOADING TABLE: ** lineorder ** IN SCHEMA ==> nodist =======\n",
      "\n",
      "                   copy lineorder from 's3://awssampledbuswest2/ssbgz/lineorder' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 628.86 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insertion twice for each schema (WARNING!! EACH CAN TAKE MORE THAN 10 MINUTES!!!)\n",
    "nodistStats = loadTables(\"nodist\", tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "======= LOADING TABLE: ** customer ** IN SCHEMA ==> dist =======\n",
      "\n",
      "                   copy customer from 's3://awssampledbuswest2/ssbgz/customer' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 25.48 sec\n",
      "\n",
      "======= LOADING TABLE: ** dwdate ** IN SCHEMA ==> dist =======\n",
      "\n",
      "                   copy dwdate from 's3://awssampledbuswest2/ssbgz/dwdate' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 1.97 sec\n",
      "\n",
      "======= LOADING TABLE: ** supplier ** IN SCHEMA ==> dist =======\n",
      "\n",
      "                   copy supplier from 's3://awssampledbuswest2/ssbgz/supplier' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 15.44 sec\n",
      "\n",
      "======= LOADING TABLE: ** part ** IN SCHEMA ==> dist =======\n",
      "\n",
      "                   copy part from 's3://awssampledbuswest2/ssbgz/part' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 18.14 sec\n",
      "\n",
      "======= LOADING TABLE: ** lineorder ** IN SCHEMA ==> dist =======\n",
      "\n",
      "                   copy lineorder from 's3://awssampledbuswest2/ssbgz/lineorder' \n",
      "                   credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
      "                   gzip region 'us-west-2';\n",
      "                   \n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "=== DONE IN: 741.38 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insertion twice for each schema (WARNING!! EACH CAN TAKE MORE THAN 10 MINUTES!!!)\n",
    "distStats = loadTables(\"dist\", tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of the timing results\n",
    "stats = distStats.join(nodistStats)\n",
    "#stats.plot.bar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loadtime_dist</th>\n",
       "      <th>loadtime_nodist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>customer</th>\n",
       "      <td>25.479078</td>\n",
       "      <td>13.978294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dwdate</th>\n",
       "      <td>1.971943</td>\n",
       "      <td>1.448280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supplier</th>\n",
       "      <td>15.436045</td>\n",
       "      <td>7.560319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>part</th>\n",
       "      <td>18.139198</td>\n",
       "      <td>5.215743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineorder</th>\n",
       "      <td>741.379035</td>\n",
       "      <td>628.864526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loadtime_dist  loadtime_nodist\n",
       "table                                    \n",
       "customer   25.479078      13.978294      \n",
       "dwdate     1.971943       1.448280       \n",
       "supplier   15.436045      7.560319       \n",
       "part       18.139198      5.215743       \n",
       "lineorder  741.379035     628.864526     "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneDim_SQL = \"\"\"\n",
    "             set enable_result_cache_for_session to off;\n",
    "             SET search_path TO {};\n",
    "\n",
    "             select sum(lo_extendedprice*lo_discount) as revenue\n",
    "             from lineorder, dwdate\n",
    "             where lo_orderdate = d_datekey\n",
    "             and d_year = 1997 \n",
    "             and lo_discount between 1 and 3 \n",
    "             and lo_quantity < 24;\n",
    "             \"\"\"\n",
    "\n",
    "twoDim_SQL = \"\"\"\n",
    "             set enable_result_cache_for_session to off;\n",
    "             SET search_path TO {};\n",
    "\n",
    "             select sum(lo_revenue), d_year, p_brand1\n",
    "             from lineorder, dwdate, part, supplier\n",
    "             where lo_orderdate = d_datekey\n",
    "             and lo_partkey = p_partkey\n",
    "             and lo_suppkey = s_suppkey\n",
    "             and p_category = 'MFGR#12'\n",
    "             and s_region = 'AMERICA'\n",
    "             group by d_year, p_brand1\n",
    "             \"\"\"\n",
    "\n",
    "drill_SQL = \"\"\"\n",
    "            set enable_result_cache_for_session to off;\n",
    "            SET search_path TO {};\n",
    "\n",
    "            select c_city, s_city, d_year, sum(lo_revenue) as revenue \n",
    "            from customer, lineorder, supplier, dwdate\n",
    "            where lo_custkey = c_custkey\n",
    "            and lo_suppkey = s_suppkey\n",
    "            and lo_orderdate = d_datekey\n",
    "            and (c_city='UNITED KI1' or\n",
    "            c_city='UNITED KI5')\n",
    "            and (s_city='UNITED KI1' or\n",
    "            s_city='UNITED KI5')\n",
    "            and d_yearmonth = 'Dec1997'\n",
    "            group by c_city, s_city, d_year\n",
    "            order by d_year asc, revenue desc;\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "oneDimSameDist_SQL = \"\"\"\n",
    "                     set enable_result_cache_for_session to off;\n",
    "                     SET search_path TO {};\n",
    "\n",
    "                     select lo_orderdate, sum(lo_extendedprice*lo_discount) as revenue  \n",
    "                     from lineorder, part\n",
    "                     where lo_partkey  = p_partkey\n",
    "                     group by lo_orderdate\n",
    "                     order by lo_orderdate\n",
    "                     \"\"\"\n",
    "\n",
    "def compareQueryTimes(schema):\n",
    "    queryTimes  =[] \n",
    "    for i, query in enumerate([oneDim_SQL, twoDim_SQL, drill_SQL, oneDimSameDist_SQL]):\n",
    "        t0 = time()\n",
    "        q = query.format(schema)\n",
    "        %sql $q\n",
    "        queryTime = time() - t0\n",
    "        queryTimes.append(queryTime)\n",
    "    return pd.DataFrame({\"query\":[\"oneDim\",\"twoDim\", \"drill\", \"oneDimSameDist\"], \"queryTime_\"+schema:queryTimes}).set_index('query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "280 rows affected.\n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "4 rows affected.\n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "2406 rows affected.\n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "280 rows affected.\n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "4 rows affected.\n",
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      "Done.\n",
      "2406 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queryTime_nodist</th>\n",
       "      <th>queryTime_dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>oneDim</th>\n",
       "      <td>13.521617</td>\n",
       "      <td>7.288541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twoDim</th>\n",
       "      <td>19.933780</td>\n",
       "      <td>15.266812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drill</th>\n",
       "      <td>16.512507</td>\n",
       "      <td>15.760254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oneDimSameDist</th>\n",
       "      <td>20.045202</td>\n",
       "      <td>13.208860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                queryTime_nodist  queryTime_dist\n",
       "query                                           \n",
       "oneDim          13.521617         7.288541      \n",
       "twoDim          19.933780         15.266812     \n",
       "drill           16.512507         15.760254     \n",
       "oneDimSameDist  20.045202         13.208860     "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noDistQueryTimes = compareQueryTimes(\"nodist\")\n",
    "distQueryTimes = compareQueryTimes(\"dist\") \n",
    "queryTimeDF = noDistQueryTimes.join(distQueryTimes)\n",
    "#queryTimeDF.plot.bar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queryTime_nodist</th>\n",
       "      <th>queryTime_dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>oneDim</th>\n",
       "      <td>13.521617</td>\n",
       "      <td>7.288541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twoDim</th>\n",
       "      <td>19.933780</td>\n",
       "      <td>15.266812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drill</th>\n",
       "      <td>16.512507</td>\n",
       "      <td>15.760254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oneDimSameDist</th>\n",
       "      <td>20.045202</td>\n",
       "      <td>13.208860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                queryTime_nodist  queryTime_dist\n",
       "query                                           \n",
       "oneDim          13.521617         7.288541      \n",
       "twoDim          19.933780         15.266812     \n",
       "drill           16.512507         15.760254     \n",
       "oneDimSameDist  20.045202         13.208860     "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryTimeDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query\n",
       "oneDim            46.097121\n",
       "twoDim            23.412359\n",
       "drill             4.555655 \n",
       "oneDimSameDist    34.104629\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improvementDF = queryTimeDF[\"distImprovement\"] = 100.0*(queryTimeDF['queryTime_nodist']-queryTimeDF['queryTime_dist'])/queryTimeDF['queryTime_nodist']\n",
    "#improvementDF.plot.bar(title=\"% dist Improvement by query\")\n",
    "#plt.show()\n",
    "improvementDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'dwh-cluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'MasterUsername': 'dwhuser',\n",
       "  'DBName': 'dwh',\n",
       "  'Endpoint': {'Address': 'dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2019, 6, 4, 18, 39, 40, 858000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-ce44eeb6',\n",
       "  'AvailabilityZone': 'us-west-2d',\n",
       "  'PreferredMaintenanceWindow': 'sun:09:00-sun:09:30',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 4,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::764499268961:role/dwh-role',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': []},\n",
       " 'ResponseMetadata': {'RequestId': '3c2c8181-8700-11e9-814d-2d365bd43db1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3c2c8181-8700-11e9-814d-2d365bd43db1',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2044',\n",
       "   'date': 'Tue, 04 Jun 2019 19:37:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '04b53606-5937-4b8a-a6ea-85ce6778724e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '04b53606-5937-4b8a-a6ea-85ce6778724e, 04b53606-5937-4b8a-a6ea-85ce6778724e',\n",
       "   'date': 'Tue, 04 Jun 2019 19:57:10 GMT',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '200'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
