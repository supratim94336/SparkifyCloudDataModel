{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "# amazon aws\n",
    "KEY = config.get('AWS', 'key')\n",
    "SECRET = config.get('AWS', 'secret')\n",
    "\n",
    "# Redshift\n",
    "DWH_DB = config.get('DWH', 'DWH_DB')\n",
    "DWH_DB_USER = config.get('DWH', 'DWH_DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('DWH', 'DWH_DB_PASSWORD')\n",
    "DWH_PORT = config.get('DWH', 'DWH_PORT')\n",
    "DWH_CLUSTER_TYPE = config.get('DWH', 'DWH_CLUSTER_TYPE')\n",
    "DWH_NUM_NODES = config.get('DWH', 'DWH_NUM_NODES')\n",
    "DWH_NODE_TYPE = config.get('DWH', 'DWH_NODE_TYPE')\n",
    "DWH_IAM_ROLE_NAME = config.get('DWH', 'DWH_IAM_ROLE_NAME')\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_SCHEMA = config.get('DWH', 'DWH_SCHEMA')\n",
    "DWH_LOG_STAGING_TABLE = config.get('DWH', 'DWH_LOG_STAGING_TABLE')\n",
    "DWH_SONG_STAGING_TABLE = config.get('DWH', 'DWH_SONG_STAGING_TABLE')\n",
    "DWH_REGION = config.get('DWH','DWH_REGION')\n",
    "\n",
    "# s3\n",
    "LOG_JSON_FORMAT = config.get('S3', 'LOG_JSON_FORMAT')\n",
    "S3_BUCKET_LOG_JSON_PATH = config.get('S3', 'S3_BUCKET_LOG_JSON_PATH')\n",
    "S3_BUCKET_SONG_JSON_PATH = config.get('S3', 'S3_BUCKET_SONG_JSON_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log_staging'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DWH_LOG_STAGING_TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from config import *\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import utils\n",
    "from smart_open import open\n",
    "\n",
    "\n",
    "def create_iam_role():\n",
    "    iam = boto3.client('iam',\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET,\n",
    "                       region_name='us-west-2'\n",
    "                       )\n",
    "    print(\"1.1 creating role\")\n",
    "    try:\n",
    "        iam.create_role(\n",
    "            Path='/',\n",
    "            RoleName=DWH_IAM_ROLE_NAME,\n",
    "            Description=\"Allows Redshift to call AWS Services.\",\n",
    "            AssumeRolePolicyDocument=json.dumps(\n",
    "                {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                  'Effect': 'Allow',\n",
    "                  'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                 'Version': '2012-10-17'})\n",
    "            )\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    print(\"1.2 Attaching Policy\")\n",
    "    try:\n",
    "        iam.attach_role_policy(\n",
    "            RoleName=DWH_IAM_ROLE_NAME,\n",
    "            PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\\\n",
    "                        ['ResponseMetadata']['HTTPStatusCode']\n",
    "    except ClientError as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    print(\"1.3 Get the IAM role ARN\")\n",
    "    roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "    return roleArn\n",
    "\n",
    "\n",
    "def create_redshift_cluster(roleArn):\n",
    "    print(\"1.1 Client is created ...\")\n",
    "    redshift = boto3.client('redshift',\n",
    "                            region_name=\"us-west-2\",\n",
    "                            aws_access_key_id=KEY,\n",
    "                            aws_secret_access_key=SECRET\n",
    "                            )\n",
    "    try:\n",
    "        print(\"1.2 Cluster config is being created ...\")\n",
    "        redshift.create_cluster(\n",
    "            # HW\n",
    "            ClusterType=DWH_CLUSTER_TYPE,\n",
    "            NodeType=DWH_NODE_TYPE,\n",
    "            NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "            # Identifiers & Credentials\n",
    "            DBName=DWH_DB,\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "            MasterUsername=DWH_DB_USER,\n",
    "            MasterUserPassword=DWH_DB_PASSWORD,\n",
    "\n",
    "            # Roles (for s3 access)\n",
    "            IamRoles=[roleArn])\n",
    "    except ClientError as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    print(\"1.3 Cluster is being created ...\")\n",
    "    while redshift.describe_clusters(\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "            ['Clusters'][0]['ClusterStatus'] != 'available':\n",
    "        utils.animate()\n",
    "\n",
    "    print(\"\\r1.4 Cluster is created successfully ...\")\n",
    "    return redshift.describe_clusters(\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "    ['Clusters'][0]['Endpoint']['Address']\n",
    "\n",
    "\n",
    "def delete_redshift_cluster():\n",
    "    print(\"1.1 Client is created ...\")\n",
    "    redshift = boto3.client('redshift',\n",
    "                            region_name=\"us-west-2\",\n",
    "                            aws_access_key_id=KEY,\n",
    "                            aws_secret_access_key=SECRET\n",
    "                            )\n",
    "    print(\"1.2 Cluster is identified ...\")\n",
    "    try:\n",
    "        redshift.delete_cluster(\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "            SkipFinalClusterSnapshot=True)\n",
    "    except ClientError as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    try:\n",
    "        print(\"1.3 Cluster is being deleted ...\")\n",
    "        while redshift.describe_clusters(\n",
    "                ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "                ['Clusters'][0]['ClusterStatus'] == 'deleting':\n",
    "            utils.animate()\n",
    "    except:\n",
    "        print(\"\\r1.4 Cluster is deleted successfully ...\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    \"\"\" Create an Amazon S3 bucket\n",
    "\n",
    "    :param bucket_name: Unique string name\n",
    "    :return: True if bucket is created, else False\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    except ClientError as e:\n",
    "        print(f'ERROR: {e}')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def upload_bucket(bucket_name, key, output_name):\n",
    "    \"\"\"\n",
    "\n",
    "    :param bucket_name: Your S3 BucketName\n",
    "    :param key: Original Name and type of the file you want to upload\n",
    "                into s3\n",
    "    :param output_name: Output file name(The name you want to give to\n",
    "                        the file after we upload to s3)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(key, bucket_name, output_name)\n",
    "\n",
    "\n",
    "def list_bucket(bucket_name, prefix):\n",
    "    \"\"\"\n",
    "\n",
    "    :param bucket_name: Your S3 BucketName\n",
    "    :param key: Original Name and type of the file you want to upload\n",
    "                into s3\n",
    "    :param output_name: Output file name(The name you want to give to\n",
    "                        the file after we upload to s3)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    s3 = boto3.resource('s3',\n",
    "                        region_name=\"us-west-2\",\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET\n",
    "                        )\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.filter(Prefix=prefix):\n",
    "        files.append(obj)\n",
    "    return files\n",
    "\n",
    "\n",
    "def s3_read(s3_path):\n",
    "    \"\"\"\n",
    "    Read a file from an S3 source.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source : str\n",
    "        Path starting with s3://, e.g. 's3://bucket-name/key/foo.bar'\n",
    "    profile_name : str, optional\n",
    "        AWS profile\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    content : bytes\n",
    "\n",
    "    botocore.exceptions.NoCredentialsError\n",
    "        Botocore is not able to find your credentials. Either specify\n",
    "        profile_name or add the environment variables AWS_ACCESS_KEY_ID,\n",
    "        AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN.\n",
    "        See https://boto3.readthedocs.io/en/latest/guide/configuration.html\n",
    "    \"\"\"\n",
    "    for line in open(s3_path, 'rb', encoding='utf-8'):\n",
    "        print(line.decode('utf8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 creating role\n",
      "ERROR: An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwh-role already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::764499268961:role/dwh-role'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_iam_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleArn = 'arn:aws:iam::764499268961:role/dwh-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Client is created ...\n",
      "1.2 Cluster config is being created ...\n",
      "1.3 Cluster is being created ...\n",
      "1.4 Cluster is created successfully ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_redshift_cluster(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sql_queries import create_table_queries, drop_table_queries\n",
    "from config import *\n",
    "import psycopg2\n",
    "import argparse\n",
    "\n",
    "\n",
    "def create_database(cur, conn):\n",
    "    \"\"\"\n",
    "    This function drops all the tables in the database\n",
    "    :param cur:\n",
    "    :param conn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cur.execute(\"CREATE SCHEMA IF NOT EXISTS {}\".format(DWH_SCHEMA))\n",
    "    conn.commit()\n",
    "    cur.execute(\"SET search_path to {}\".format(DWH_SCHEMA))\n",
    "    conn.commit()\n",
    "    return None\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function drops all the tables in the database\n",
    "    :param cur:\n",
    "    :param conn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cur.execute(\"SET search_path to {}\".format(DWH_SCHEMA))\n",
    "    conn.commit()\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function creates all the tables in the database\n",
    "    :param cur:\n",
    "    :param conn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cur.execute(\"SET search_path to {}\".format(DWH_SCHEMA))\n",
    "    conn.commit()\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP TABLES\n",
    "# ----------------------------------------------------------------------\n",
    "log_staging_table_drop = \"DROP TABLE IF EXISTS log_staging CASCADE\"\n",
    "song_staging_table_drop = \"DROP TABLE IF EXISTS song_staging CASCADE\"\n",
    "songplay_table_drop = \"DROP TABLE IF EXISTS songplays CASCADE;\"\n",
    "user_table_drop = \"DROP TABLE IF EXISTS users;\"\n",
    "song_table_drop = \"DROP TABLE IF EXISTS songs;\"\n",
    "artist_table_drop = \"DROP TABLE IF EXISTS artists;\"\n",
    "time_table_drop = \"DROP TABLE IF EXISTS time;\"\n",
    "\n",
    "# CREATE TABLES\n",
    "# ----------------------------------------------------------------------\n",
    "log_staging_table_create = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS log_staging (\n",
    "    artist VARCHAR(50), \n",
    "    auth VARCHAR(50), \n",
    "    firstName VARCHAR(50), \n",
    "    gender VARCHAR(10), \n",
    "    iteminSession INTEGER, \n",
    "    lastName VARCHAR(50), \n",
    "    length NUMERIC, \n",
    "    level VARCHAR(10), \n",
    "    location VARCHAR(100), \n",
    "    method VARCHAR(10),\n",
    "    page VARCHAR(50), \n",
    "    registration NUMERIC, \n",
    "    sessionId INTEGER, \n",
    "    song VARCHAR(50),\n",
    "    status INTEGER,\n",
    "    ts TIMESTAMP,\n",
    "    userAgent VARCHAR(100),\n",
    "    userId INTEGER);    \n",
    "\"\"\"\n",
    "\n",
    "song_staging_table_create = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS song_staging (\n",
    "    num_songs INTEGER, \n",
    "    artist_id VARCHAR(50), \n",
    "    artist_latitude NUMERIC, \n",
    "    artist_longitude NUMERIC, \n",
    "    artist_location VARCHAR(100), \n",
    "    artist_name VARCHAR(100), \n",
    "    song_id NUMERIC, \n",
    "    title VARCHAR(50), \n",
    "    duration NUMERIC, \n",
    "    year INTEGER);    \n",
    "\"\"\"\n",
    "\n",
    "# facts ----------------------------------------------------------------\n",
    "songplay_table_create = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS songplays (\n",
    "    songplay_id INTEGER IDENTITY(0,1) PRIMARY KEY, \n",
    "    start_time TIMESTAMP NOT NULL REFERENCES time(start_time) sortkey, \n",
    "    user_id VARCHAR(50) NOT NULL REFERENCES users(user_id), \n",
    "    level VARCHAR(10) NOT NULL, \n",
    "    song_id VARCHAR(50) NOT NULL REFERENCES songs(song_id) distkey, \n",
    "    artist_id VARCHAR(50) NOT NULL REFERENCES artists(artist_id), \n",
    "    session_id INTEGER NOT NULL, \n",
    "    location VARCHAR(100) NOT NULL, \n",
    "    user_agent VARCHAR(50) NOT NULL);\n",
    "\"\"\"\n",
    "\n",
    "# dimensions -----------------------------------------------------------\n",
    "user_table_create = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id VARCHAR(50) PRIMARY KEY sortkey, \n",
    "    first_name VARCHAR(50), \n",
    "    last_name VARCHAR(50), \n",
    "    gender VARCHAR(10), \n",
    "    level VARCHAR(10) NOT NULL)\n",
    "    diststyle ALL;\n",
    "\"\"\"\n",
    "\n",
    "song_table_create = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS songs (\n",
    "    song_id VARCHAR(50) PRIMARY KEY distkey, \n",
    "    title VARCHAR(100) NOT NULL, \n",
    "    artist_id VARCHAR(50) NOT NULL, \n",
    "    year INTEGER NOT NULL,\n",
    "    duration NUMERIC NOT NULL);\n",
    "\"\"\"\n",
    "\n",
    "artist_table_create = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS artists (\n",
    "    artist_id VARCHAR(50) PRIMARY KEY sortkey, \n",
    "    name VARCHAR(100) NOT NULL, \n",
    "    location VARCHAR NOT NULL, \n",
    "    latitude NUMERIC NOT NULL, \n",
    "    longitude NUMERIC NOT NULL)\n",
    "    diststyle ALL;\n",
    "\"\"\"\n",
    "\n",
    "time_table_create = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS time (\n",
    "    start_time TIMESTAMP UNIQUE NOT NULL sortkey, \n",
    "    hour INTEGER NOT NULL, \n",
    "    day INTEGER NOT NULL, \n",
    "    week INTEGER NOT NULL, \n",
    "    month INTEGER NOT NULL, \n",
    "    year INTEGER NOT NULL, \n",
    "    week_day VARCHAR)\n",
    "    diststyle ALL;\n",
    "\"\"\"\n",
    "\n",
    "# INSERT RECORDS\n",
    "# ----------------------------------------------------------------------\n",
    "songplay_table_insert = (\"\"\"\n",
    "INSERT INTO songplays (start_time, user_id, level, song_id, artist_id,\n",
    " session_id, location, user_agent) \n",
    " SELECT DISTINCT lgs.ts, \n",
    "                 lsg.userId, \n",
    "                 nvl(lgs.level, 'empty'), \n",
    "                 ssg.song_id, \n",
    "                 lsg.artistId,\n",
    "                 lsg.sessionId, \n",
    "                 nvl(lgs.location, 'empty'), \n",
    "                 nvl(lgs.userAgent, 'empty')\n",
    " FROM log_staging lgs\n",
    " INNER JOIN song_staging ssg ON lgs.song = ssg.title\n",
    " WHERE lgs.page = 'NextSong';\n",
    "\"\"\")\n",
    "\n",
    "user_table_insert = (\"\"\"\n",
    "INSERT INTO users (user_id, first_name, last_name, gender, level) \n",
    "  SELECT DISTINCT lgs.userId, \n",
    "                  nvl(lgs.firstName, 'empty'), \n",
    "                  nvl(lgs.lastName, 'empty'),  \n",
    "                  nvl(lgs.gender, 'empty'),  \n",
    "                  nvl(lgs.level, 'empty'), \n",
    "  FROM log_staging lgs\n",
    "  WHERE lgs.userId IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "song_table_insert = (\"\"\"\n",
    "INSERT INTO songs (song_id, title, artist_id, year, duration) \n",
    " SELECT DISTINCT ssg.song_id, \n",
    "                 ssg.title, \n",
    "                 ssg.artist_id, \n",
    "                 ssg.year, \n",
    "                 nvl(ssg.duration, 0.0)\n",
    "  FROM song_staging ssg\n",
    "\"\"\")\n",
    "\n",
    "artist_table_insert = (\"\"\"\n",
    "INSERT INTO artists (artist_id, name, location, latitude, longitude) \n",
    " SELECT DISTINCT ssg.artist_id, \n",
    "                 ssg.artist_name, \n",
    "                 nvl(ssg.artist_location, 'empty'), \n",
    "                 nvl(ssg.artist_latitude, 0.0), \n",
    "                 nvl(ssg.artist_longitude, 0.0)\n",
    " FROM song_staging ssg\n",
    " WHERE ssg.artist_id IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "time_table_insert = (\"\"\"\n",
    "INSERT INTO time (start_time, hour, day, week, month, year, week_day)\n",
    " SELECT DISTINCT se.ts, \n",
    "                 DATE_PART(hour, se.ts) :: INTEGER, \n",
    "                 DATE_PART(day, se.ts) :: INTEGER, \n",
    "                 DATE_PART(week, se.ts) :: INTEGER,\n",
    "                 DATE_PART(month, se.ts) :: INTEGER,\n",
    "                 DATE_PART(year, se.ts) :: INTEGER,\n",
    "                 DATE_PART(dow, se.ts) :: INTEGER\n",
    " FROM log_staging lsg\n",
    " WHERE lsg.page = 'NextSong';\n",
    "\"\"\")\n",
    "\n",
    "# FIND SONGS\n",
    "# you'll need to get the song ID and artist ID by querying the songs\n",
    "# and artists tables to find matches based on song title, artist name,\n",
    "# and song duration time\n",
    "song_select = (\"\"\"\n",
    "SELECT s.song_id, s.artist_id FROM songs s\n",
    " JOIN artists a ON s.artist_id=a.artist_id\n",
    " WHERE s.title = %s AND a.name=%s AND s.duration=%s;\n",
    "\"\"\")\n",
    "\n",
    "# QUERY LISTS\n",
    "\n",
    "create_table_queries = [log_staging_table_create,\n",
    "                        song_staging_table_create,\n",
    "                        user_table_create,\n",
    "                        song_table_create,\n",
    "                        artist_table_create,\n",
    "                        time_table_create,\n",
    "                        songplay_table_create]\n",
    "insert_table_queries = [user_table_insert,\n",
    "                        song_table_insert,\n",
    "                        artist_table_insert,\n",
    "                        time_table_insert,\n",
    "                        songplay_table_insert]\n",
    "drop_table_queries = [log_staging_table_drop,\n",
    "                      song_staging_table_drop,\n",
    "                      songplay_table_drop,\n",
    "                      user_table_drop,\n",
    "                      song_table_drop,\n",
    "                      artist_table_drop,\n",
    "                      time_table_drop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = 'dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com'\n",
    "\n",
    "# create postgres connection\n",
    "conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(\n",
    "                DWH_DB_USER,\n",
    "                DWH_DB_PASSWORD,\n",
    "                DWH_ENDPOINT,\n",
    "                DWH_PORT,\n",
    "                DWH_DB\n",
    ")\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cur = conn.cursor()\n",
    "create_database(cur, conn)\n",
    "drop_tables(cur, conn)\n",
    "create_tables(cur, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import psycopg2\n",
    "import argparse\n",
    "from sql_queries import *\n",
    "\n",
    "\n",
    "def process_data_staging(cur, conn, iam_role):\n",
    "    \"\"\"\n",
    "    i/p: cursor, connection, filepath and ETL function\n",
    "    returns: None\n",
    "    This is a helper function for extracting, transforming and loading\n",
    "    data onto the relational database\n",
    "    \"\"\"\n",
    "    cur.execute(\"SET search_path to {}\".format(DWH_SCHEMA))\n",
    "    conn.commit()\n",
    "    copy_log_command = \"\"\"\n",
    "                        copy {}.{} from '{}' credentials 'aws_iam_role={}' emptyasnull blanksasnull format as json '{}' region 'us-west-2' timeformat 'auto';\n",
    "                        \"\"\".format(DWH_SCHEMA, DWH_LOG_STAGING_TABLE, S3_BUCKET_LOG_JSON_PATH, iam_role, LOG_JSON_FORMAT)\n",
    "    print(copy_log_command)\n",
    "    cur.execute(copy_log_command)\n",
    "    conn.commit()\n",
    "    cur.execute(\"SET search_path to {}\".format(DWH_SCHEMA))\n",
    "    conn.commit()\n",
    "    copy_song_command = \"\"\"\n",
    "                        copy {}.{} from '{}' credentials 'aws_iam_role={}' emptyasnull blanksasnull json 'auto' region 'us-west-2' timeformat 'auto';\n",
    "                            \"\"\".format(DWH_SCHEMA, DWH_SONG_STAGING_TABLE, S3_BUCKET_SONG_JSON_PATH,\n",
    "                                       iam_role)\n",
    "    cur.execute(copy_song_command)\n",
    "    conn.commit()\n",
    "    return None\n",
    "\n",
    "\n",
    "def insert_data_into_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    This function creates all the tables in the database\n",
    "    :param cur:\n",
    "    :param conn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cur.execute(\"SET search_path to {}\".format(DWH_SCHEMA))\n",
    "    conn.commit()\n",
    "    for query in insert_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = 'dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com'\n",
    "iam_role = 'arn:aws:iam::764499268961:role/dwh-role'\n",
    "\n",
    "# create postgres connection\n",
    "conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(\n",
    "    DWH_DB_USER,\n",
    "    DWH_DB_PASSWORD,\n",
    "    DWH_ENDPOINT,\n",
    "    DWH_PORT,\n",
    "    DWH_DB\n",
    ")\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cur = conn.cursor()\n",
    "copy_song_command = \"\"\"\n",
    "                    copy {}.{} from '{}' iam_role 'arn:aws:iam::764499268961:role/dwh-role' emptyasnull blanksasnull json 'auto' region 'us-west-2' timeformat 'auto';\n",
    "                    commit;\"\"\".format(DWH_SCHEMA, DWH_SONG_STAGING_TABLE, S3_BUCKET_SONG_JSON_PATH)\n",
    "print(copy_song_command)\n",
    "cur.execute(copy_song_command)\n",
    "conn.commit()\n",
    "#process_data_staging(cur, conn, iam_role)\n",
    "# insert_data_into_tables(conn, cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "copy sparkify.song_staging \n",
    "from 's3://udacity-dend/song_data/' \n",
    "credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role' \n",
    "emptyasnull \n",
    "blanksasnull \n",
    "json 'auto' \n",
    "region 'us-west-2' \n",
    "timeformat 'auto';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "copy sparkify.log_staging\n",
    "from 's3://udacity-dend/log_data/2018' \n",
    "credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
    "emptyasnull\n",
    "blanksasnull\n",
    "format as json 's3://udacity-dend/log_json_path.json'\n",
    "timeformat 'auto';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM stl_load_errors LIMIT 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sql_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from config import *\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "\n",
    "iam = boto3.client('iam', aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET,\n",
    "                       region_name='us-west-2'\n",
    "                       )\n",
    "try:\n",
    "    iam.create_role(Path='/',\n",
    "                    RoleName=DWH_IAM_ROLE_NAME,\n",
    "                    Description=\"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "                    AssumeRolePolicyDocument=json.dumps(\n",
    "                        {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                          'Effect': 'Allow',\n",
    "                          'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                         'Version': '2012-10-17'})\n",
    "                    )\n",
    "\n",
    "except ClientError as e:\n",
    "    print(f'ERROR: {e}')\n",
    "\n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                       )['ResponseMetadata']['HTTPStatusCode']\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Client is created ...\n",
      "1.2 Cluster config is being created ...\n",
      "1.3 Cluster is being created ...\n",
      "1.4 Cluster is created successfully ...\n"
     ]
    }
   ],
   "source": [
    "print(\"1.1 Client is created ...\")\n",
    "redshift = boto3.client('redshift',\n",
    "                        region_name=\"us-west-2\",\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET\n",
    "                        )\n",
    "try:\n",
    "    print(\"1.2 Cluster config is being created ...\")\n",
    "    redshift.create_cluster(\n",
    "        # HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        # Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "\n",
    "        # Roles (for s3 access)\n",
    "        IamRoles=[roleArn])\n",
    "except ClientError as e:\n",
    "    print(f'ERROR: {e}')\n",
    "\n",
    "print(\"1.3 Cluster is being created ...\")\n",
    "while redshift.describe_clusters(\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "        ['Clusters'][0]['ClusterStatus'] != 'available':\n",
    "    utils.animate()\n",
    "\n",
    "print(\"\\r1.4 Cluster is created successfully ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = redshift.describe_clusters(\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "    ['Clusters'][0]['Endpoint']['Address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT, DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS sparkify.log_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "CREATE TABLE IF NOT EXISTS sparkify.log_staging (\n",
    "        artist VARCHAR, \n",
    "        auth VARCHAR, \n",
    "        firstName VARCHAR, \n",
    "        gender VARCHAR, \n",
    "        itemInSession VARCHAR, \n",
    "        lastName VARCHAR, \n",
    "        length VARCHAR, \n",
    "        level VARCHAR, \n",
    "        location VARCHAR, \n",
    "        method VARCHAR,\n",
    "        page VARCHAR, \n",
    "        registration VARCHAR, \n",
    "        sessionId VARCHAR, \n",
    "        song VARCHAR,\n",
    "        status VARCHAR,\n",
    "        ts VARCHAR,\n",
    "        userAgent VARCHAR,\n",
    "        userId VARCHAR); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS sparkify.log_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "copy sparkify.log_staging\n",
    "from 's3://udacity-dend/log_data/2018' \n",
    "credentials 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role'\n",
    "emptyasnull\n",
    "blanksasnull\n",
    "json 'auto'\n",
    "timeformat 'auto';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>artist</th>\n",
       "        <th>auth</th>\n",
       "        <th>firstname</th>\n",
       "        <th>gender</th>\n",
       "        <th>iteminsession</th>\n",
       "        <th>lastname</th>\n",
       "        <th>length</th>\n",
       "        <th>level</th>\n",
       "        <th>location</th>\n",
       "        <th>method</th>\n",
       "        <th>page</th>\n",
       "        <th>registration</th>\n",
       "        <th>sessionid</th>\n",
       "        <th>song</th>\n",
       "        <th>status</th>\n",
       "        <th>ts</th>\n",
       "        <th>useragent</th>\n",
       "        <th>userid</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM sparkify.log_staging LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql COPY sparkify.log_staging FROM 's3://udacity-dend/log_data/2018' iam_role 'aws_iam_role=arn:aws:iam::764499268961:role/dwh-role' region 'us-west-2' FORMAT AS JSON 's3://udacity-dend/log_json_path.json' timeformat 'epochmillisecs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    " CREATE TABLE IF NOT EXISTS sparkify.log_staging (\n",
    "    artist VARCHAR(50), \n",
    "    auth VARCHAR(50), \n",
    "    firstname VARCHAR(50), \n",
    "    gender VARCHAR(10), \n",
    "    iteminsession INTEGER, \n",
    "    lastname VARCHAR(50), \n",
    "    length NUMERIC, \n",
    "    level VARCHAR(10), \n",
    "    location VARCHAR(100), \n",
    "    method VARCHAR(10),\n",
    "    page VARCHAR(50), \n",
    "    registration NUMERIC, \n",
    "    sessionid INTEGER, \n",
    "    song VARCHAR(50),\n",
    "    status INTEGER,\n",
    "    ts TIMESTAMP,\n",
    "    useragent VARCHAR(100),\n",
    "    userid INTEGER);    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM log_staging LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM STL_LOAD_ERRORS ORDER BY starttime DESC LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Client is created ...\n",
      "1.2 Cluster is identified ...\n",
      "1.3 Cluster is being deleted ...\n",
      "1.4 Cluster is deleted successfully ...\n"
     ]
    }
   ],
   "source": [
    "print(\"1.1 Client is created ...\")\n",
    "redshift = boto3.client('redshift',\n",
    "                        region_name=\"us-west-2\",\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET\n",
    "                        )\n",
    "print(\"1.2 Cluster is identified ...\")\n",
    "try:\n",
    "    redshift.delete_cluster(\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        SkipFinalClusterSnapshot=True)\n",
    "except ClientError as e:\n",
    "    print(f'ERROR: {e}')\n",
    "\n",
    "try:\n",
    "    print(\"1.3 Cluster is being deleted ...\")\n",
    "    while redshift.describe_clusters(\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\\\n",
    "            ['Clusters'][0]['ClusterStatus'] == 'deleting':\n",
    "        utils.animate()\n",
    "except:\n",
    "    print(\"\\r1.4 Cluster is deleted successfully ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.cgjrwscs7tjx.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "7 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>table_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>artists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>log_staging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>song_staging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>songplays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>songs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>users</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('artists',),\n",
       " ('log_staging',),\n",
       " ('song_staging',),\n",
       " ('songplays',),\n",
       " ('songs',),\n",
       " ('time',),\n",
       " ('users',)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select t.table_name\n",
    "from information_schema.tables t\n",
    "where t.table_schema = 'sparkify'\n",
    "order by t.table_name;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
